{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbadb21e",
   "metadata": {},
   "source": [
    "# Lecture 17: Quasi-Newton methods\n",
    "\n",
    "## Approximation of the derivative\n",
    "\n",
    "-   The formula $x^{(i+1)} = x^{(i)} - \\frac{f(x^{(i)})}{f'(x^{(i)})}$ requires that we are able to compute an expression for the derivative of $f(x)$.\n",
    "\n",
    "-   This may not always be possible:\n",
    "\n",
    "    -   the function $f(x)$ may be a \"black box\";\n",
    "    -   the formula for $f(x)$ may be known but may be difficult for us to differentiate.\n",
    "\n",
    "-   We can modify Newton's method by simply approximating $f'(x^{(i)})$, rather like we approximated $y'(t)$ when solving a differential equation.\n",
    "\n",
    "### Approximation of $f'(x)$\n",
    "\n",
    "-   Recall that $f'(x) = \\lim_{\\mathrm{d}x \\to 0} \\frac{f(x + \\mathrm{d}x) - f(x)}{\\mathrm{d}x}$.\n",
    "\n",
    "-   Hence we can choose a small value for $\\mathrm{d}x$ (how small?) and approximate:\n",
    "\n",
    "    $$\n",
    "    f'(x) \\approx \\frac{f(x + \\mathrm{d}x) - f(x)}{\\mathrm{d}x}.\n",
    "    $$\n",
    "\n",
    "-   This modified-Newton method then becomes\n",
    "\n",
    "    $$\n",
    "    x^{(i+1)} = x^{(i)} - \\frac{\\mathrm{d}x \\times f(x^{(i)})}{f(x^{(i)} + \\mathrm{d}x) - f(x^{(i)})}.\n",
    "    $$\n",
    "\n",
    "### The choice of $\\mathrm{d}x$\n",
    "\n",
    "-   Note that the computational cost of calculating each iterate is about the same as for Newton's method.\n",
    "\n",
    "-   What is not obvious however is what choice should be made for the value of $\\mathrm{d}x$.\n",
    "\n",
    "-   In theory (exact arithmetic) the smaller the choice of $\\mathrm{d}x$ the better the approximation of the derivative.\n",
    "\n",
    "-   In practice, however, we know that the operation of subtracting two very similar values from each other can lead to significant rounding errors when using floating point arithmetic.\n",
    "\n",
    "-   Consider the following example...\n",
    "\n",
    "## Problems with floating point arithmetic\n",
    "\n",
    "-   When $f(x) = x^3$ then $f'(x) = 3 x^2$.\n",
    "\n",
    "-   Hence, at $x_0 = 1$, $f(x_0) = 1$ and $f'(x_0) = 3$.\n",
    "\n",
    "-   Consider what happens when we approximate this with python, using finite values for $\\mathrm{d}x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15820330",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_27a01f08_986a_11ee_b5da_0022488db4ce\" ><caption>Simple approximation of a derivative using floating point arithmetic</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >dx</th>        <th class=\"col_heading level0 col1\" >approx</th>        <th class=\"col_heading level0 col2\" >abs error</th>        <th class=\"col_heading level0 col3\" >rel error</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow0_col0\" class=\"data row0 col0\" >1.000000e-04</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow0_col1\" class=\"data row0 col1\" >3.000300</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow0_col2\" class=\"data row0 col2\" >3.000100e-04</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow0_col3\" class=\"data row0 col3\" >1.000033e-04</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow1_col0\" class=\"data row1 col0\" >1.000000e-06</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow1_col1\" class=\"data row1 col1\" >3.000003</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow1_col2\" class=\"data row1 col2\" >2.999798e-06</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow1_col3\" class=\"data row1 col3\" >9.999326e-07</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow2_col0\" class=\"data row2 col0\" >1.000000e-08</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow2_col1\" class=\"data row2 col1\" >3.000000</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow2_col2\" class=\"data row2 col2\" >3.972048e-09</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow2_col3\" class=\"data row2 col3\" >1.324016e-09</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow3_col0\" class=\"data row3 col0\" >1.000000e-10</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow3_col1\" class=\"data row3 col1\" >3.000000</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow3_col2\" class=\"data row3 col2\" >2.482211e-07</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow3_col3\" class=\"data row3 col3\" >8.274037e-08</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow4_col0\" class=\"data row4 col0\" >1.000000e-12</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow4_col1\" class=\"data row4 col1\" >3.000267</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow4_col2\" class=\"data row4 col2\" >2.667017e-04</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow4_col3\" class=\"data row4 col3\" >8.890058e-05</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow5_col0\" class=\"data row5 col0\" >1.000000e-14</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow5_col1\" class=\"data row5 col1\" >2.997602</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow5_col2\" class=\"data row5 col2\" >2.397834e-03</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow5_col3\" class=\"data row5 col3\" >7.992778e-04</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow6_col0\" class=\"data row6 col0\" >1.000000e-16</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow6_col1\" class=\"data row6 col1\" >0.000000</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow6_col2\" class=\"data row6 col2\" >3.000000e+00</td>\n",
       "                        <td id=\"T_27a01f08_986a_11ee_b5da_0022488db4cerow6_col3\" class=\"data row6 col3\" >1.000000e+00</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f4eb510abb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**3\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    return 3 * x**2\n",
    "\n",
    "\n",
    "x = 1\n",
    "\n",
    "headers = [\"dx\", \"approx\", \"abs error\", \"rel error\"]\n",
    "data = []\n",
    "\n",
    "for e in range(4, 18, 2):\n",
    "    dx = 10**-e\n",
    "    approx = (f(x + dx) - f(x)) / dx\n",
    "    exact = df(x)\n",
    "    abs_error = abs(exact - approx)\n",
    "    rel_error = abs(exact - approx) / exact\n",
    "\n",
    "    new_data = [dx, approx, abs_error, rel_error]\n",
    "    data.append(new_data)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "df.style.format(\n",
    "    formatter={\"dx\": \"{:e}\", \"approx\": \"{:f}\", \"abs error\": \"{:e}\", \"rel error\": \"{:e}\"}\n",
    ").hide_index().set_caption(\n",
    "    \"Simple approximation of a derivative using floating point arithmetic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439991f",
   "metadata": {},
   "source": [
    "## Modified Newton's method\n",
    "\n",
    "-   Recall the definition of machine precision/unit roundoff from Lecture 3.\n",
    "\n",
    "-   The modified Newton method uses $\\mathrm{d}x = \\sqrt{eps}$.\n",
    "\n",
    "- For double precision we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164fb44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx=1.4901161193847656e-08\n",
      "df_approx=3.0000000447034836\n",
      "abs_error=4.470348358154297e-08\n",
      "rel_error=1.4901161193847656e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "dx = np.sqrt(eps)\n",
    "\n",
    "x0 = 1.0\n",
    "df_approx = ((x0 + dx) ** 3 - x0**3) / dx\n",
    "abs_error = abs(df_approx - 3)\n",
    "rel_error = abs_error / 3\n",
    "\n",
    "print(f\"{dx=}\\n{df_approx=}\\n{abs_error=}\\n{rel_error=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fcae7b",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "- The example to compute the square root of 2 to a tolerance of $10^{-12}$ with starting value $1$ gives $x^* \\approx 1.4142135623731$ after 5 iterations.\n",
    "\n",
    "- The naca0012 example starting at 1 with tolerance $10^{-4}$ gives $x^* \\approx 0.76579$ after 2 iterations.\n",
    "\n",
    "- The naca0012 example starting at 1 with tolerance $10^{-5}$ gives $x^* \\approx 0.765239$ after 3 iterations.\n",
    "\n",
    "- The naca0012 example starting at 0.1 with tolerance $10^{-4}$ gives $x^* \\approx 0.03386$ after 5 iterations.\n",
    "\n",
    "In each case the performance is almost identical to the Newton method.\n",
    "\n",
    "## The secant method\n",
    "\n",
    "-   Suppose we choose $\\mathrm{d}x = x^{(i-1)} - x^{(i)}$, then we get\n",
    "\n",
    "    $$\n",
    "    f'(x^{(i)}) \\approx \\frac{f(x^{(i)} + \\mathrm{d}x) - f(x^{(i)})}{\\mathrm{d}x}\n",
    "    = \\frac{f(x^{(i)}) - f(x^{(i-1)})}{x^{(i)} - x^{(i-1)}}.\n",
    "    $$\n",
    "\n",
    "-   Newton's method then becomes\n",
    "\n",
    "    $$\n",
    "    x^{(i+1)} = x^{(i)} - f(x^{(i)}) \\frac{x^{(i)} - x^{(i-1)}}{f(x^{i}) - f(x^{(i-1)})}\n",
    "    \\left(\\approx x^{(i)} - \\frac{f(x^{(i)})}{f'(x^{(i)})} \\right).\n",
    "    $$\n",
    "\n",
    "-   Note that the main advantage of this approach over the previous modified Newton approximation is that only one new evaluation of $f(x)$ is required per iteration (apart from the very first iteration).\n",
    "\n",
    "### Pros and cons\n",
    "\n",
    "Advantages:\n",
    "\n",
    "-   $f'(x)$ is not required;\n",
    "-   only one new evaluation of $f(x)$ per iteration;\n",
    "-   still converges almost as quickly as Newton's method.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "-   *two* starting values, $x^{(0)}$ and $x^{(1)}$, are required;\n",
    "-   may require one more iteration than exact Newton (but iterations are cheaper);\n",
    "-   like Newton's method the iteration can fail to converge for some starting iterates.\n",
    "\n",
    "### Examples\n",
    "\n",
    "- The example to compute the square root of 2 starting from 1 and 1.5 to a tolerance of $10^{-4}$ gives the solution as $x^* \\approx 1.4142$ after 3 iterations.\n",
    "\n",
    "- The example to computation compound interest start from 100 and 120 to a tolerance of 0.1 gives the solution as $x^* \\approx 235.9$ after 6 iterations.\n",
    "\n",
    "- The naca0012 example starting from 1 and 0.9 to a tolerance of $10^{-4}$ gives the solution as $x^* \\approx 0.7653$ after 3 iterations.\n",
    "\n",
    "- The naca0012 example starting from 0 and 0.1 to a tolerance of $10^{-4}$ gives the solution as $x^* \\approx 0.0339$ after 5 iterations.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "- Wikipedia: [Quasi-Newton method](https://en.wikipedia.org/wiki/Quasi-Newton_method)\n",
    "- Wikipedia: [Secant method](https://en.wikipedia.org/wiki/Secant_method)\n",
    "- `scipy`: Optimization and root finding [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/optimize.html)\n",
    "\n",
    "The [slides used in the lecture](./lec17_.ipynb) are also available"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.15.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "source_map": [
   13,
   64,
   98,
   108,
   120
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}